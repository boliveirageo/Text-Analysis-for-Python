{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonte: https://appliedmachinelearning.blog/2018/01/18/conventional-approach-to-text-classification-clustering-using-k-nearest-neighbor-k-means-python-implementation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, scipy, nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando as funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatText(text):\n",
    "    #Esta função tem o objetivo de formatar o texto para o classificador K Nearest Neighborhud - KNN\n",
    "    # 1 - Converter string em lista\n",
    "    # 2.1 - Remover as stopwords\n",
    "    # 2.2 - Remover as pontuações\n",
    "    # 3 - Remover os sufixos das palavras\n",
    "        \n",
    "    #Retirando espacos e colando as palavras minuscula\n",
    "    wpt = WordPunctTokenizer()\n",
    "    text = np.array(text)\n",
    "    \n",
    "    \n",
    "    #text = (text.str.lower()).apply(wpt.tokenize)#wpt.tokenize(text.lower()) \n",
    "    text = map(wpt.tokenize,text)    \n",
    "    #Retirando as stopwords e as pontuacoes\n",
    "    #Coletando as stopword em Português\n",
    "    stopw = stopwords.words('english') + list(punctuation)\n",
    "    \n",
    "    #Retirando as stopwords\n",
    "    palavras_sem_stopwords = []\n",
    "    for word_final in text:\n",
    "        palavras_sem_stopwords.append([word for word in word_final if word not in stopw])\n",
    "\n",
    "    #Retirando os sufixos das palavras\n",
    "    stm = PorterStemmer()\n",
    "    texto_final_matrix = list()\n",
    "    for texto in palavras_sem_stopwords:    \n",
    "        texto_final = [stm.stem(txt.lower()) for txt in texto]\n",
    "        texto_final_matrix.append(texto_final)\n",
    "    \n",
    "    text = texto_final_matrix\n",
    "\n",
    "    listText = [] \n",
    "    for row in text:\n",
    "        a = ''\n",
    "        for txt in row:\n",
    "            a += ' '+txt\n",
    "        listText.append(a)\n",
    "    \n",
    "    text = listText\n",
    "    return text\n",
    "\n",
    "#Criar um dataframe a partir de dados em formato .txt\n",
    "def preProcessingCorpus(folder):\n",
    "\n",
    "    #Criando o arquivo\n",
    "    files = glob.glob(os.path.join(folder,'*.txt'))\n",
    "    \n",
    "    #Criando o nova dataframe\n",
    "    df = pd.DataFrame()\n",
    "    count = 1   \n",
    "    #Percorrendo todos os arquivo\n",
    "    for file in files:      \n",
    "    \n",
    "        #Coletando informacoes do contidas no arquivo e nome do arquivo com docid\n",
    "        data = open(file,'r',encoding='utf-8')\n",
    "        docid = str(os.path.basename(file)[0:-4])\n",
    "            \n",
    "        #Inserindo valores no dataframe      \n",
    "        datas =pd.DataFrame({'id':[count],'docid':[docid],'text':[data.read()],'class1':[0],'class2':[0]})\n",
    "        \n",
    "        df = df.append(datas)\n",
    "        datas = None\n",
    "    \n",
    "        #Fechando o arquivo\n",
    "        data.close()\n",
    "        count += 1\n",
    "    \n",
    "    return df\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leitura do arquivo\n",
    "foldersrc = r'<local dos arquivos de treino>'\n",
    "os.chdir(foldersrc)\n",
    "\n",
    "#Criando o nova dataframe\n",
    "df = pd.DataFrame()\n",
    "count = 1\n",
    "\n",
    "for x in os.listdir('.'):\n",
    "    if os.path.isdir(x):\n",
    "        \n",
    "        #Criando o folder\n",
    "        subfolder = os.path.join(foldersrc,os.path.join(x,'*.txt'))\n",
    "               \n",
    "        #Criando o arquivo\n",
    "        files = glob.glob(subfolder)\n",
    "       \n",
    "        #Percorrendo todos os arquivo\n",
    "        for file in files:      \n",
    "    \n",
    "            #Coletando informacoes do contidas no arquivo e nome do arquivo com docid\n",
    "            data = open(file,'r',encoding='utf-8')\n",
    "            docid = str(os.path.basename(file)[0:-4])\n",
    "            \n",
    "            #Inserindo valores no dataframe      \n",
    "            datas =pd.DataFrame({'id':[count],'docid':[docid],'text':[data.read()],'class':[int(x)-1]})\n",
    "        \n",
    "            df = df.append(datas)\n",
    "            datas = None\n",
    "    \n",
    "            #Fechando o arquivo\n",
    "            data.close()\n",
    "            count += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando amostras e rótulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatando o texto\n",
    "df['text'] = formatText(df['text'])\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#Separando as variaveis independentes das dependentes\n",
    "independentes = vectorizer.fit_transform(df['text'])\n",
    "dependentes = np.array(df['class'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifcando textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(independentes,dependentes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preditar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = r'<pasta do arquivo para preditar>'\n",
    "df_pred_data = preProcessingCorpus(src)\n",
    "df_pred_data['text'] = formatText(df_pred_data['text'])\n",
    "pred_data = vectorizer.transform(df_pred_data['text'])\n",
    "labels_preditos = knn.predict_proba(pred_data)\n",
    "rows,cols = labels_preditos.shape\n",
    "df_pred_data['class1'] = labels_preditos[np.array(range(rows)),0] #Class 1\n",
    "df_pred_data['class2'] = labels_preditos[np.array(range(rows)),1] #Class 2\n",
    "\n",
    "writer = pd.ExcelWriter('<saida do arquivo>')\n",
    "df_pred_data.to_excel(writer,'<nome da planilha>')\n",
    "writer.save()\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
